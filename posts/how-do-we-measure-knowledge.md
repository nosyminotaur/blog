---
title: 'How do we measure knowledge?'
date: '02 June, 2022'
excerpt: "People love to rant about how bad tests are. Curious, isn’t it, how all these systems seem to fail in the same way?"
readQuantity: '3 min read'
---

Think about it. It’s tough. In schools and colleges, the primary medium to measure knowledge is through multiple choice questions, or through written exams, part of them containing questions requiring rote memory.

I’ve done my graduation from BITS Pilani, where we used to have these open-book tests with no multiple choice questions, only problems to solve.

The problems were long, had few questions besides describing the problem and maybe a few questions to guide the student along the path to solving it. We had either 3 or 4 hours to solve those problems.

Those tests worked very well. I'd come out from one of those tests having often learned something new.

There were some courses where tests involved multiple choice questions. They were closed books with questions around rote memory. While I did feel that some of the education in the US was valuable and interesting, I hated those tests, they didn't correlate as much with comprehension of the subject matter and more with learning facts that are more or less tangentially related to the subject matter. I still remember in a computer graphics tests being shocked by being asked when OpenGL was first released, which companies were involved and other completely useless knowledge.

In open book tests, it is close to impossible to cheat, but MCQs were pretty much made for cheating. So in my opinion, cheating is a symptom of bad tests.

It's easy for us to start berating the educational system by saying how bad it is today, based on a rewards system where one to memorizes the most will be on top. But we're forgetting that it's tough to create open book tests, requiring manual efforts and time. Just shifting the question data a little gives room to scale these quizzes and is much more easy to check answers to.

This is a very tricky space to deal with.

We have deemed it necessary to create assessments to prop up the idea that education can be easily measured and should bring meaningful life outcomes for most people. Most if not all "cheating" behaviour is either just a rational, strategic response to this situation, or a disconnect between how people actually solve problems (e.g. often collaborative and laser-focused on the part of the problem that drives the outcome, in this case the assessment) and the notion of what it means for an individual to do it "correctly".

For better or worse, whether somebody completed their degree does often factor into hiring decisions, so exam grades do indirectly factor into hiring decisions. Having completed a degree signals some level of domain knowledge, conscientiousness, and intelligence. Without exams you would have a 100% success rate (unless you introduced some other assessment mechanism), so the signal would be gone; having completed a degree would only signal whether or not you were able to afford it financially.

Criticising the existing system is easy. Giving an alternative is harder. Implementing that alternative and showing that it's actually better on some metric is MUCH harder than that.

Can you think of a solution?